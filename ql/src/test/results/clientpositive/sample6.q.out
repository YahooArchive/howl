query: CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE
query: -- both input pruning and sample filter
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest1 SELECT s.* 
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 4 on key) s
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_TABREF srcbucket (TOK_TABLESAMPLE 1 4 (TOK_TABLE_OR_COL key)) s)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF s)))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-4 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-4

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        s 
            Filter Operator
              predicate:
                  expr: (((hash(key) & 2147483647) % 4) = 0)
                  type: boolean
              Filter Operator
                predicate:
                    expr: (((hash(key) & 2147483647) % 4) = 0)
                    type: boolean
                Select Operator
                  expressions:
                        expr: key
                        type: string
                        expr: value
                        type: string
                  Select Operator
                    expressions:
                          expr: UDFToInteger(_col0)
                          type: int
                          expr: _col1
                          type: string
                    File Output Operator
                      compressed: false
                      GlobalTableId: 1
                      directory: file:/data/users/pchakka/workspace/oshive/build/ql/tmp/557388751/10002
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          properties:
                            name dest1
                            columns.types int:string
                            serialization.ddl struct dest1 { i32 key, string value}
                            serialization.format 1
                            columns key,value
                            bucket_count -1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                            file.inputformat org.apache.hadoop.mapred.TextInputFormat
                            file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                            location file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/dest1
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          name: dest1
      Needs Tagging: false
      Path -> Alias:
        file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcbucket/kv1.txt 
      Path -> Partition:
        file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcbucket/kv1.txt 
          Partition
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                name srcbucket
                columns.types string:string
                bucket_field_name key
                serialization.ddl struct srcbucket { string key, string value}
                columns key,value
                serialization.format 1
                bucket_count 2
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                file.inputformat org.apache.hadoop.mapred.TextInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                location file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcbucket
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: srcbucket

  Stage: Stage-4
    Conditional Operator
      list of dependent Tasks:
          Move Operator
            files:
                hdfs directory: true
                source: file:/data/users/pchakka/workspace/oshive/build/ql/tmp/557388751/10002
                destination: file:/data/users/pchakka/workspace/oshive/build/ql/tmp/1329298090/10000
          Map Reduce
            Alias -> Map Operator Tree:
              file:/data/users/pchakka/workspace/oshive/build/ql/tmp/557388751/10002 
                  Reduce Output Operator
                    sort order: 
                    Map-reduce partition columns:
                          expr: rand()
                          type: double
                    tag: -1
                    value expressions:
                          expr: key
                          type: int
                          expr: value
                          type: string
            Needs Tagging: false
            Path -> Alias:
              file:/data/users/pchakka/workspace/oshive/build/ql/tmp/557388751/10002 
            Path -> Partition:
              file:/data/users/pchakka/workspace/oshive/build/ql/tmp/557388751/10002 
                Partition
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      name dest1
                      columns.types int:string
                      serialization.ddl struct dest1 { i32 key, string value}
                      serialization.format 1
                      columns key,value
                      bucket_count -1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      file.inputformat org.apache.hadoop.mapred.TextInputFormat
                      file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                      location file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/dest1
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: dest1
            Reduce Operator Tree:
              Extract
                File Output Operator
                  compressed: false
                  GlobalTableId: 0
                  directory: file:/data/users/pchakka/workspace/oshive/build/ql/tmp/1329298090/10000
                  table:
                      input format: org.apache.hadoop.mapred.TextInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                      properties:
                        name dest1
                        columns.types int:string
                        serialization.ddl struct dest1 { i32 key, string value}
                        serialization.format 1
                        columns key,value
                        bucket_count -1
                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        file.inputformat org.apache.hadoop.mapred.TextInputFormat
                        file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        location file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/dest1
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      name: dest1

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          source: file:/data/users/pchakka/workspace/oshive/build/ql/tmp/1329298090/10000
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                name dest1
                columns.types int:string
                serialization.ddl struct dest1 { i32 key, string value}
                serialization.format 1
                columns key,value
                bucket_count -1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                file.inputformat org.apache.hadoop.mapred.TextInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                location file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/dest1
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: dest1
          tmp directory: file:/data/users/pchakka/workspace/oshive/build/ql/tmp/1329298090/10001


query: INSERT OVERWRITE TABLE dest1 SELECT s.* 
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 4 on key) s
Input: default/srcbucket
Output: default/dest1
query: SELECT dest1.* FROM dest1
Input: default/dest1
Output: file:/data/users/pchakka/workspace/oshive/build/ql/tmp/569046726/10000
165	val_165
484	val_484
150	val_150
224	val_224
66	val_66
213	val_213
374	val_374
495	val_495
37	val_37
327	val_327
15	val_15
338	val_338
459	val_459
466	val_466
396	val_396
309	val_309
367	val_367
0	val_0
455	val_455
316	val_316
345	val_345
129	val_129
378	val_378
4	val_4
356	val_356
169	val_169
125	val_125
437	val_437
286	val_286
187	val_187
176	val_176
459	val_459
51	val_51
103	val_103
239	val_239
213	val_213
176	val_176
275	val_275
260	val_260
404	val_404
217	val_217
84	val_84
466	val_466
8	val_8
411	val_411
172	val_172
129	val_129
158	val_158
0	val_0
26	val_26
165	val_165
327	val_327
51	val_51
404	val_404
95	val_95
282	val_282
187	val_187
316	val_316
169	val_169
77	val_77
0	val_0
118	val_118
282	val_282
419	val_419
15	val_15
118	val_118
19	val_19
224	val_224
309	val_309
389	val_389
327	val_327
242	val_242
392	val_392
242	val_242
396	val_396
95	val_95
11	val_11
143	val_143
228	val_228
33	val_33
103	val_103
367	val_367
239	val_239
480	val_480
202	val_202
316	val_316
235	val_235
80	val_80
44	val_44
466	val_466
257	val_257
190	val_190
114	val_114
396	val_396
217	val_217
125	val_125
187	val_187
480	val_480
491	val_491
305	val_305
444	val_444
169	val_169
323	val_323
480	val_480
136	val_136
172	val_172
462	val_462
26	val_26
462	val_462
341	val_341
183	val_183
84	val_84
37	val_37
448	val_448
194	val_194
477	val_477
169	val_169
400	val_400
