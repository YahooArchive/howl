query: CREATE TABLE dest1(a array<int>, b array<string>, c map<string,string>, d int, e string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '1'
COLLECTION ITEMS TERMINATED BY '2'
MAP KEYS TERMINATED BY '3'
LINES TERMINATED BY '10'
STORED AS TEXTFILE
query: EXPLAIN
FROM src_thrift
INSERT OVERWRITE TABLE dest1 SELECT src_thrift.lint, src_thrift.lstring, src_thrift.mstringstring, src_thrift.aint, src_thrift.astring
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_TABREF src_thrift)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL src_thrift) lint)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL src_thrift) lstring)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL src_thrift) mstringstring)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL src_thrift) aint)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL src_thrift) astring)))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-4 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-4

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        src_thrift 
          TableScan
            alias: src_thrift
            Select Operator
              expressions:
                    expr: lint
                    type: array<int>
                    expr: lstring
                    type: array<string>
                    expr: mstringstring
                    type: map<string,string>
                    expr: aint
                    type: int
                    expr: astring
                    type: string
              outputColumnNames: _col0, _col1, _col2, _col3, _col4
              File Output Operator
                compressed: false
                GlobalTableId: 1
                table:
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: dest1

  Stage: Stage-4
    Conditional Operator
      list of dependent Tasks:
          Move Operator
            files:
                hdfs directory: true
                destination: file:/data/users/athusoo/commits/hive_trunk_ws1/build/ql/tmp/1925142463/10000
          Map Reduce
            Alias -> Map Operator Tree:
              file:/data/users/athusoo/commits/hive_trunk_ws1/build/ql/tmp/1140895264/10002 
                  Reduce Output Operator
                    sort order: 
                    Map-reduce partition columns:
                          expr: rand()
                          type: double
                    tag: -1
                    value expressions:
                          expr: a
                          type: array<int>
                          expr: b
                          type: array<string>
                          expr: c
                          type: map<string,string>
                          expr: d
                          type: int
                          expr: e
                          type: string
            Reduce Operator Tree:
              Extract
                File Output Operator
                  compressed: false
                  GlobalTableId: 0
                  table:
                      input format: org.apache.hadoop.mapred.TextInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      name: dest1

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: dest1


query: FROM src_thrift
INSERT OVERWRITE TABLE dest1 SELECT src_thrift.lint, src_thrift.lstring, src_thrift.mstringstring, src_thrift.aint, src_thrift.astring
Input: default/src_thrift
Output: default/dest1
query: SELECT dest1.* FROM dest1
Input: default/dest1
Output: file:/data/users/athusoo/commits/hive_trunk_ws1/build/ql/tmp/842958925/10000
[0,0,0]	["0","0","0"]	{"key_0":"value_0"}	1712634731	record_0
[1,2,3]	["10","100","1000"]	{"key_1":"value_1"}	465985200	record_1
[2,4,6]	["20","200","2000"]	{"key_2":"value_2"}	-751827638	record_2
[3,6,9]	["30","300","3000"]	{"key_3":"value_3"}	477111222	record_3
[4,8,12]	["40","400","4000"]	{"key_4":"value_4"}	-734328909	record_4
[5,10,15]	["50","500","5000"]	{"key_5":"value_5"}	-1952710710	record_5
[6,12,18]	["60","600","6000"]	{"key_6":"value_6"}	1244525190	record_6
[7,14,21]	["70","700","7000"]	{"key_7":"value_7"}	-1461153973	record_7
[8,16,24]	["80","800","8000"]	{"key_8":"value_8"}	1638581578	record_8
[9,18,27]	["90","900","9000"]	{"key_9":"value_9"}	336964413	record_9
null	null	null	0	NULL
query: SELECT dest1.a[0], dest1.b[0], dest1.c['key2'], dest1.d, dest1.e FROM dest1
Input: default/dest1
Output: file:/data/users/athusoo/commits/hive_trunk_ws1/build/ql/tmp/1145493551/10000
0	0	NULL	1712634731	record_0
1	10	NULL	465985200	record_1
2	20	NULL	-751827638	record_2
3	30	NULL	477111222	record_3
4	40	NULL	-734328909	record_4
5	50	NULL	-1952710710	record_5
6	60	NULL	1244525190	record_6
7	70	NULL	-1461153973	record_7
8	80	NULL	1638581578	record_8
9	90	NULL	336964413	record_9
NULL	NULL	NULL	0	NULL
