<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  See the NOTICE file distributed with
  this work for additional information regarding copyright ownership.
  The ASF licenses this file to You under the Apache License, Version 2.0
  (the "License"); you may not use this file except in compliance with
  the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V2.0//EN" "http://forrest.apache.org/dtd/document-v20.dtd">

<document>
  <header>
    <title>Howl Input and Output Interfaces</title>
  </header>
  <body>

 <!-- ==================================================================== --> 
  <section>
  <title>Set Up</title>
  <p>No Howl-specific setup is required for the HowlInputFormat and HowlOutputFormat interfaces.</p>
  <p></p>
<p><strong>Authentication</strong></p>
<table>
	<tr>
	<td><p>If a failure results in a message like "2010-11-03 16:17:28,225 WARN hive.metastore ... - Unable to connect metastore with URI thrift://..." in /tmp/&lt;username&gt;/hive.log, then make sure you have run "kinit &lt;username&gt;@&lt;YOUR KERBEROS KEY DISTRIBUTOR&gt;" to get a kerberos ticket and to be able to authenticate to the Howl server. </p></td>
	</tr>
</table>
  </section>

<!-- ==================================================================== -->
<section>
	<title>HowlInputFormat</title>
	<p>The HowlInputFormat is used with MapReduce jobs to read data from Howl managed tables.</p> 
	<p>HowlInputFormat exposes a new Hadoop 20 MapReduce API for reading data as if it had been published to a table. If a MapReduce job uses this InputFormat to write output, the default InputFormat configured for the table is used as the underlying InputFormat and the new partition is published to the table after the job completes. </p> 
	
<section>
	<title>API</title>
	<p>The API exposed by HowlInputFormat is shown below.</p>
	
	<p>To use HowlInputFormat to read data, first instantiate a <code>HowlTableInfo</code> with the necessary information from the table being read 
	and then call setInput on the <code>HowlInputFormat</code>.</p>

<p>You can use the <code>setOutputSchema</code> method to include a projection schema, to specify specific output fields. If a schema is not specified, this default to the table level schema.</p>

<p>You can use the <code>getTableSchema</code> methods to determine the table schema for a specified input table.</p>
	
	
<source>
    /**
     * Set the input to use for the Job. This queries the metadata server with
     * the specified partition predicates, gets the matching partitions, puts
     * the information in the conf object. The inputInfo object is updated with
     * information needed in the client context
     * @param job the job object
     * @param inputInfo the table input info
     * @throws IOException the exception in communicating with the metadata server
     */
    public static void setInput(Job job, HowlTableInfo inputInfo) throws IOException;

    /**
     * Set the schema for the HowlRecord data returned by HowlInputFormat.
     * @param job the job object
     * @param howlSchema the schema to use as the consolidated schema
     */
    public static void setOutputSchema(Job job,HowlSchema howlSchema) throws Exception;

    /**
     * Gets the HowlTable schema for the table specified in the HowlInputFormat.setInput call
     * on the specified job context. This information is available only after HowlInputFormat.setInput
     * has been called for a JobContext.
     * @param context the context
     * @return the table schema
     * @throws Exception if HowlInputFromat.setInput has not been called for the current context
     */
    public static HowlSchema getTableSchema(JobContext context) throws Exception	
</source>
	
</section>
</section>    
 
 
<!-- ==================================================================== -->      
<section>
	<title>HowlOutputFormat</title>
	<p>HowlOutputFormat is used with MapReduce jobs to write data to Howl managed tables.</p> 
	
	<p>HowlOutputFormat exposes a new Hadoop 20 MapReduce API for writing data to a table. If a MapReduce job uses this OutputFormat to write output, the default OutputFormat configured for the table is used as the underlying OutputFormat and the new partition is published to the table after the job completes. </p>
	
<section>
	<title>API</title>
	<p>The API exposed by HowlOutputFormat is shown below.</p>
	<p>The first call on the HowlOutputFormat must be <code>setoutput</code>; any other call will throw an exception saying the output format is not initialized. The schema for the data being written out is specified by the <code>setSchema </code> method. If this is not called on the HowlOutputFormat, then by default it is assumed that the the partition has the same schema as the current table level schema. </p>
	
<source>
/**
     * Set the info about the output to write for the Job. This queries the metadata server
     * to find the StorageDriver to use for the table.  Throws error if partition is already published.
     * @param job the job object
     * @param outputInfo the table output info
     * @throws IOException the exception in communicating with the metadata server
     */
    public static void setOutput(Job job, HowlTableInfo outputInfo) throws IOException;

    /**
     * Set the schema for the data being written out to the partition. The
     * table schema is used by default for the partition if this is not called.
     * @param job the job object
     * @param schema the schema for the data
     * @throws IOException the exception
     */
    public static void setSchema(Job job, HowlSchema schema) throws IOException;

    /**
     * Gets the table schema for the table specified in the HowlOutputFormat.setOutput call
     * on the specified job context.
     * @param context the context
     * @return the table schema
     * @throws IOException if HowlOutputFromat.setOutput has not been called for the passed context
     */
    public static HowlSchema getTableSchema(JobContext context) throws IOException
</source>
</section>   

<section>
	<title>Partition Schema Semantics</title>
	
	<p>The partition schema specified can be different from the current table level schema. The rules about what kinds of schema are allowed are:</p>
	
	<ul>
	<li>If a column is present in both the table schema and the partition schema, the type for the column should match. 
</li>
	<li>If the partition schema has lesser columns that the table level schema, then only the columns at the end of the table schema are allowed to be absent. Columns in the middle cannot be absent. So if table schema is "c1,c2,c3", partition schema can be "c1" or "c1,c2" but not "c1,c3" or "c2,c3"</li>
	<li>If the partition schema has extra columns, then the extra columns should appear after the table schema. So if table schema is "c1,c2", the partition schema can be "c1,c2,c3" but not "c1,c3,c4". The table schema is automatically updated to have the extra column. In the previous example, the table schema will become "c1,c2,c3" after the completion of the job. 
</li>
	<li>The partition keys are not allowed to be present in the schema being written out. 
</li>
	</ul>
	
</section>   
</section>

  </body>
</document>
