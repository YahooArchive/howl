<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  See the NOTICE file distributed with
  this work for additional information regarding copyright ownership.
  The ASF licenses this file to You under the Apache License, Version 2.0
  (the "License"); you may not use this file except in compliance with
  the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V2.0//EN" "http://forrest.apache.org/dtd/document-v20.dtd">

<document>
  <header>
    <title>Overview </title>
  </header>
  <body>
   <section>
      <title>Howl </title>
      
       <p>Howl is a table management and storage management layer for Hadoop that enables users with different data processing tools – Pig, MapReduce, Hive, Streaming – to more easily read and write data on the grid. Howl’s table abstraction presents users with a relational view of data in the Hadoop distributed file system (HDFS) and ensures that users need not worry about where or in what format their data is stored – RCFile format, text files, sequence files. </p>
<p>(Note: In this release, Streaming is not supported. Also, Howl supports only writing RCFile formatted files and only reading PigStorage formated text files.)</p>
<p></p>
<figure src="images/howl-product.jpg" align="left" alt="Howl Product"/>


</section>
      
      
      <section>
      <title>Howl Architecture</title>
      <p>Howl is built on top of the Hive metastore and incorporates components from the Hive DDL. Howl provides read and write interfaces for Pig and MapReduce and a command line interface for data definitions.</p>
<p>(Note: Howl notification is not available in this release.)</p>

<figure src="images/howl-archt.jpg" align="left" alt="Howl Architecture"/>

<p></p>

<section>
<title>Interfaces</title>   
<p>The Howl interface for Pig – HowlLoader and HowlStorer – is an implementation of the Pig load and store interfaces. HowlLoader accepts a table to read data from; you can indicate which partitions to scan by immediately following the load statement with a partition filter statement. HowlStorer accepts a table to write to and a specification of partition keys to create a new partition. Currently HowlStorer only supports writing to one partition. HowlLoader and HowlStorer are implemented on top of HowlInputFormat and HowlOutputFormat respectively (see <a href="loadstore.html">Howl Load and Store</a>).</p>

<p>The Howl interface for MapReduce – HowlInputFormat and HowlOutputFormat – is an implementation of Hadoop InputFormat and OutputFormat. HowlInputFormat accepts a table to read data from and a selection predicate to indicate which partitions to scan. HowlOutputFormat accepts a table to write to and a specification of partition keys to create a new partition. Currently HowlOutputFormat only supports writing to one partition (see <a href="inputoutput.html">Howl Input and Output</a>).</p>

<p><strong>Note:</strong> Currently there is no Hive-specific interface. Since Howl uses Hive's metastore, Hive can read data in Howl directly as long as a SerDe for that data already exists. In the future we plan to write a HowlSerDe so that users won't need storage-specific SerDes and so that Hive users can write data to Howl.</p>

<p>Data is defined using Howl's command line interface (CLI). The Howl CLI supports the DDL portion of Hive's query language, allowing users to create, alter, drop tables, etc. The CLI also supports the data exploration part of the Hive command line, such as SHOW TABLES, DESCRIBE TABLE, etc. (see the <a href="cli.html">Howl Command Line Interface</a>).</p> 
</section>

<section>
<title>Data Model</title>
<p>Howl presents a relational view of data in HDFS. Data is stored in tables and these tables can be placed in databases. Tables can also be hash partitioned on one or more keys; that is, for a given value of a key (or set of keys) there will be one partition that contains all rows with that value (or set of values). For example, if a table is partitioned on date and there are three days of data in the table, there will be three partitions in the table. New partitions can be added to a table, and partitions can be dropped from a table. Partitioned tables have no partitions at create time. Unpartitioned tables effectively have one default partition that must be created at table creation time. There is no guaranteed read consistency when a partition is dropped.</p>

<p>Partitions contain records. Once a partition is created records cannot be added to it, removed from it, or updated in it. (In the future some ability to integrate changes to a partition will be added.) Partitions are multi-dimensional and not hierarchical. Records are divided into columns. Columns have a name and a datatype. Howl supports the same datatypes as Hive (see <a href="loadstore.html">Howl Load and Store</a>). </p>
</section>
     </section>
     
  <section>
  <title>Data Flow Example</title>
  <p>This simple data flow example shows how Howl is used to move data from the grid into a database. 
  From the database, the data can then be analyzed using Hive.</p>
  
 <p><strong>First</strong> Joe in data acquisition uses distcp to get data onto the grid.</p>
 <source>
hadoop distcp file:///file.dat hdfs://data/rawevents/20100819/data

howl “alter table rawevents add partition 20100819 hdfs://data/rawevents/20100819/data”
</source>
  
<p><strong>Second</strong> Sally in data processing uses Pig to cleanse and prepare the data.</p>  
<p>Without Howl, Sally must be manually informed by Joe that data is available, or use Oozie and poll on HDFS.</p>
<source>
A = load ‘/data/rawevents/20100819/data’ as (alpha:int, beta:chararray, …);
B = filter A by bot_finder(zeta) = 0;
…
store Z into ‘data/processedevents/20100819/data’;
</source>

<p>With Howl, Oozie will be notified by Howl data is available and can then start the Pig job</p>
<source>
A = load ‘rawevents’ using HowlLoader;
B = filter A by date = ‘20100819’ and by bot_finder(zeta) = 0;
…
store Z into ‘processedevents’ using HowlStorer(“date=20100819”);
</source>

<p><strong>Third</strong> Robert in client management uses Hive to analyze his clients' results.</p>
<p>Without Howl, Robert must alter the table to add the required partition. </p>
 <source>
alter table processedevents add partition 20100819 hdfs://data/processedevents/20100819/data

select advertiser_id, count(clicks)
from processedevents
where date = ‘20100819’ 
group by adverstiser_id;
</source> 
<p>With Howl, Robert does not need to modify the table structure.</p>
 <source>
select advertiser_id, count(clicks)
from processedevents
where date = ‘20100819’ 
group by adverstiser_id;
</source>

</section>
  
  
  </body>
</document>
